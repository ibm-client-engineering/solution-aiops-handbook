[
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Best Practices\n(Capture the main takeaways and results of the project)"
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html",
    "href": "src/implementation_methodology/steptwo-imp.html",
    "title": "Step Two",
    "section": "",
    "text": "Step Two Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/solution_overview/configuration.html",
    "href": "src/solution_overview/configuration.html",
    "title": "AIOps on Linux Configuration",
    "section": "",
    "text": "To enable a secure connection (LDAPS) between AIOps and an external LDAP server, you must import the LDAP server‚Äôs Certificate Authority (CA) certificate into the platform‚Äôs truststore.\n\n\nBefore applying the configuration, it is important to understand why this certificate is needed. There are two common patterns for integrating LDAP with AIOps using SAML.\n\n\nIn this model, the IdP handles all LDAP communication. AIOps learns about user and group data through SAML login attempts.\n\nCertificate Requirement: The LDAP CA is in the IdP connection to LDAP, not AIOps.\nAIOps Config: No LDAP connection required in AIOps.\n\n\n\n\n\n\nflowchart TD\n    User((User))\n    AIOps[\"AIOps Platform\"]\n    IdP[\"IdP\"]\n    LDAP[(\"LDAP Server\")]\n\n    subgraph \"Authorization Flow\"\n    User --&gt;|\"(1) Access UI\"| AIOps\n    AIOps -.-&gt;|\"(2) Redirect for Auth\"| IdP\n    IdP &lt;--&gt;|\"(3) Verify Creds and Fetch Groups\"| LDAP\n    IdP == \"(4) SAML Token (User + Groups)\" ==&gt; AIOps\n    end\n\n    style AIOps fill:#14164a,stroke:#333,stroke-width:2px\n    style LDAP fill:#eee,stroke:#333,stroke-width:2px\n    \n    linkStyle 3 stroke-width:4px,fill:none,stroke:green;\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nSimpler Config: AIOps needs no knowledge of LDAP topology.\nRole Management: AIOps must first wait to be told about a group from a failed SAML login, then that group can be assigned a role.\n\n\nSingle Trust Store: Only the IdP needs the LDAP certificates.\n\n\n\n\n\n\n\n\nIn this model, AIOps uses SAML for authentication but connects directly to LDAP to search for user groups and details. The instructions in this document apply to this scenario.\n\nCertificate Requirement: You must import the LDAP CA into AIOps.\nAIOps Config: Requires an LDAP connection profile in AIOps.\n\n\n\n\n\n\nflowchart TD\n    User((User))\n    AIOps[\"AIOps Platform\"]\n    IdP[\"IdP\"]\n    LDAP[(\"LDAP Server\")]\n\n    subgraph \"Authorization Flow\"\n    User --&gt;|\"(1) Access UI\"| AIOps\n    AIOps -.-&gt;|\"(2) Redirect for Auth\"| IdP\n    IdP &lt;--&gt;|\"(3) Verify Creds Only\"| LDAP\n    IdP == \"(4) SAML Token (User Only)\" ==&gt; AIOps\n    AIOps &lt;--&gt;|\"(5) Direct Query: Get User Groups\"| LDAP\n    end\n\n    style AIOps fill:#f9f,stroke:#333,stroke-width:2px\n    style LDAP fill:#eee,stroke:#333,stroke-width:2px\n    \n    linkStyle 3 stroke-width:4px,fill:none,stroke:green;\n    linkStyle 4 stroke-width:4px,fill:none,stroke:red;\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nRole Management: You can manage all the roles of LDAP users and groups inside the AIOps console immediately.\nComplexity: Requires configuring LDAP in two places (IdP and AIOps).\n\n\n\nNetwork: AIOps requires direct firewall access to the LDAP server.\n\n\n\n\n\n\n\n\n\nAccess: kubectl access to the cluster.\nCertificate: The CA certificate file (e.g., ca.crt) that signed your LDAP server‚Äôs certificate.\n\nNote: This must be the Root CA of the LDAP server, not an ingress certificate or a client certificate.\n\n\n\n\n\n\n\nThe certificate must be converted to a single-line Base64 string to be stored in a Kubernetes Secret.\nRun the following command on your infrastructure node (Linux):\n# Replace 'ca.crt' with your actual filename\ncat ca.crt | base64 -w 0\n\n\n\n\n\n\nTip\n\n\n\nMac Users: If you are running this on macOS, use cat ca.crt | base64 (omit the -w 0).\n\n\nCopy the output string to your clipboard. It will look like a long random string (e.g., LS0tLS1CRUdJTi...).\n\n\n\nAIOps uses a specific secret named platform-auth-ldaps-ca-cert to store trusted LDAP certificates.\n\nOpen the secret for editing:\nkubectl edit secret platform-auth-ldaps-ca-cert -n aiops\nLocate the data section.\nFind the key named certificate.\n\nIf the key exists: Replace the existing value.\nIf the key is empty/missing: Add certificate: followed by your string.\n\nIt should look like this:\napiVersion: v1\ndata:\n  certificate: &lt;PASTE_YOUR_BASE64_STRING_HERE&gt;\nkind: Secret\nmetadata:\n  name: platform-auth-ldaps-ca-cert\n  namespace: aiops\ntype: Opaque\nSave and exit the editor (usually Esc then :wq if using vi).\n\n\n\n\nThe authentication pods do not automatically reload the secret. You must restart them to pick up the new certificate.\nkubectl delete pod -l component=platform-auth-service -n aiops\nWait for the pods to come back up to the Running state:\nkubectl get pods -l component=platform-auth-service -n aiops -w\n\n\n\nOnce the pods are running, you can test the connection via the AIOps Console or by checking the logs.\nTo check the logs for successful connection attempts:\nkubectl logs -n aiops -l component=platform-auth-service -f\n\n\n\n\n\n\n\n\n\n\nLog Error\nProbable Cause\n\n\n\n\nETIMEDOUT\nFirewall / Network: The pod cannot reach the LDAP IP on port 636.\n\n\nHandshake failed\nCertificate: The CA cert in the secret is wrong or expired.\n\n\nPKIX path building failed\nTrust: The server provided a cert that the secret‚Äôs CA does not sign.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/configuration.html#architecture-patterns",
    "href": "src/solution_overview/configuration.html#architecture-patterns",
    "title": "AIOps on Linux Configuration",
    "section": "",
    "text": "Before applying the configuration, it is important to understand why this certificate is needed. There are two common patterns for integrating LDAP with AIOps using SAML.\n\n\nIn this model, the IdP handles all LDAP communication. AIOps learns about user and group data through SAML login attempts.\n\nCertificate Requirement: The LDAP CA is in the IdP connection to LDAP, not AIOps.\nAIOps Config: No LDAP connection required in AIOps.\n\n\n\n\n\n\nflowchart TD\n    User((User))\n    AIOps[\"AIOps Platform\"]\n    IdP[\"IdP\"]\n    LDAP[(\"LDAP Server\")]\n\n    subgraph \"Authorization Flow\"\n    User --&gt;|\"(1) Access UI\"| AIOps\n    AIOps -.-&gt;|\"(2) Redirect for Auth\"| IdP\n    IdP &lt;--&gt;|\"(3) Verify Creds and Fetch Groups\"| LDAP\n    IdP == \"(4) SAML Token (User + Groups)\" ==&gt; AIOps\n    end\n\n    style AIOps fill:#14164a,stroke:#333,stroke-width:2px\n    style LDAP fill:#eee,stroke:#333,stroke-width:2px\n    \n    linkStyle 3 stroke-width:4px,fill:none,stroke:green;\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nSimpler Config: AIOps needs no knowledge of LDAP topology.\nRole Management: AIOps must first wait to be told about a group from a failed SAML login, then that group can be assigned a role.\n\n\nSingle Trust Store: Only the IdP needs the LDAP certificates.\n\n\n\n\n\n\n\n\nIn this model, AIOps uses SAML for authentication but connects directly to LDAP to search for user groups and details. The instructions in this document apply to this scenario.\n\nCertificate Requirement: You must import the LDAP CA into AIOps.\nAIOps Config: Requires an LDAP connection profile in AIOps.\n\n\n\n\n\n\nflowchart TD\n    User((User))\n    AIOps[\"AIOps Platform\"]\n    IdP[\"IdP\"]\n    LDAP[(\"LDAP Server\")]\n\n    subgraph \"Authorization Flow\"\n    User --&gt;|\"(1) Access UI\"| AIOps\n    AIOps -.-&gt;|\"(2) Redirect for Auth\"| IdP\n    IdP &lt;--&gt;|\"(3) Verify Creds Only\"| LDAP\n    IdP == \"(4) SAML Token (User Only)\" ==&gt; AIOps\n    AIOps &lt;--&gt;|\"(5) Direct Query: Get User Groups\"| LDAP\n    end\n\n    style AIOps fill:#f9f,stroke:#333,stroke-width:2px\n    style LDAP fill:#eee,stroke:#333,stroke-width:2px\n    \n    linkStyle 3 stroke-width:4px,fill:none,stroke:green;\n    linkStyle 4 stroke-width:4px,fill:none,stroke:red;\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nRole Management: You can manage all the roles of LDAP users and groups inside the AIOps console immediately.\nComplexity: Requires configuring LDAP in two places (IdP and AIOps).\n\n\n\nNetwork: AIOps requires direct firewall access to the LDAP server.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/configuration.html#prerequisites",
    "href": "src/solution_overview/configuration.html#prerequisites",
    "title": "AIOps on Linux Configuration",
    "section": "",
    "text": "Access: kubectl access to the cluster.\nCertificate: The CA certificate file (e.g., ca.crt) that signed your LDAP server‚Äôs certificate.\n\nNote: This must be the Root CA of the LDAP server, not an ingress certificate or a client certificate.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/configuration.html#procedure",
    "href": "src/solution_overview/configuration.html#procedure",
    "title": "AIOps on Linux Configuration",
    "section": "",
    "text": "The certificate must be converted to a single-line Base64 string to be stored in a Kubernetes Secret.\nRun the following command on your infrastructure node (Linux):\n# Replace 'ca.crt' with your actual filename\ncat ca.crt | base64 -w 0\n\n\n\n\n\n\nTip\n\n\n\nMac Users: If you are running this on macOS, use cat ca.crt | base64 (omit the -w 0).\n\n\nCopy the output string to your clipboard. It will look like a long random string (e.g., LS0tLS1CRUdJTi...).\n\n\n\nAIOps uses a specific secret named platform-auth-ldaps-ca-cert to store trusted LDAP certificates.\n\nOpen the secret for editing:\nkubectl edit secret platform-auth-ldaps-ca-cert -n aiops\nLocate the data section.\nFind the key named certificate.\n\nIf the key exists: Replace the existing value.\nIf the key is empty/missing: Add certificate: followed by your string.\n\nIt should look like this:\napiVersion: v1\ndata:\n  certificate: &lt;PASTE_YOUR_BASE64_STRING_HERE&gt;\nkind: Secret\nmetadata:\n  name: platform-auth-ldaps-ca-cert\n  namespace: aiops\ntype: Opaque\nSave and exit the editor (usually Esc then :wq if using vi).\n\n\n\n\nThe authentication pods do not automatically reload the secret. You must restart them to pick up the new certificate.\nkubectl delete pod -l component=platform-auth-service -n aiops\nWait for the pods to come back up to the Running state:\nkubectl get pods -l component=platform-auth-service -n aiops -w\n\n\n\nOnce the pods are running, you can test the connection via the AIOps Console or by checking the logs.\nTo check the logs for successful connection attempts:\nkubectl logs -n aiops -l component=platform-auth-service -f\n\n\n\n\n\n\n\n\n\n\nLog Error\nProbable Cause\n\n\n\n\nETIMEDOUT\nFirewall / Network: The pod cannot reach the LDAP IP on port 636.\n\n\nHandshake failed\nCertificate: The CA cert in the secret is wrong or expired.\n\n\nPKIX path building failed\nTrust: The server provided a cert that the secret‚Äôs CA does not sign.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/configuration.html#setting-up-a-promethues-alertmanager-webhook-in-aiops",
    "href": "src/solution_overview/configuration.html#setting-up-a-promethues-alertmanager-webhook-in-aiops",
    "title": "AIOps on Linux Configuration",
    "section": "Setting Up a Promethues AlertManager Webhook in AIOps",
    "text": "Setting Up a Promethues AlertManager Webhook in AIOps\n\n1. Define the Webhook in the AIOps UI\n\nNavigate to Integrations in the AIOps console and select Add integration.\nUnder the Events category, select Prometheus AlertManager, click Get started.\nProvide a Name (e.g.¬†Prometheus) and optional description for the webhook to identify its purpose (e.g., Prometheus Alerts (Self Monitoring)).\nSelect None for Authentication type and click Next.\n\n\n\n\n2. Map Prometheus Alert JSON to AIOps Schema\n\nIn the webhook configuration screen, locate the Mapping section.\nUse the following JSONata mapping:\n\n(\n    /* Set resource based on labels available */\n    $resource := function($labels){(\n      $name := $labels.name ? $labels.name\n        : $labels.node_name ? $labels.node_name\n        : $labels.statefulset ? $labels.statefulset\n        : $labels.deployment ? $labels.deployment\n        : $labels.daemonset ? $labels.daemonset\n        : $labels.pod ? $labels.pod\n        : $labels.container ? $labels.container\n        : $labels.instance ? $labels.instance\n        : $labels.app ? $labels.app\n        : $labels.job_name ? $labels.job_name\n        : $labels.job ? $labels.job\n        : $labels.type ? $labels.type: $labels.prometheus;\n\n      /* Conditional Namespace Append */\n      $namespace_appended := $labels.namespace ? ($name & '/' & $labels.namespace) : $name;\n\n      /* Check if the determined $name is likely a node/hardware name */\n      $is_node_alert := $labels.node_name or $labels.instance;\n\n      $is_node_alert ? $name : $namespace_appended; /* Only append if NOT a node alert */\n    )};    \n    /* Map to event schema */\n    alerts.(\n      { \n        \"summary\": annotations.summary ? annotations.summary: annotations.description ? annotations.description : annotations.message ? annotations.message,\n        \"severity\": $lowercase(labels.severity) = \"critical\" ? 6 : $lowercase(labels.severity) = \"major\" ? 5 : $lowercase(labels.severity) = \"minor\" ? 4 : $lowercase(labels.severity) = \"warning\" ? 3 : 1, \n        \"resource\": {\n          \"name\": $resource(labels)\n        },\n        \"type\": {\n          \"eventType\": $lowercase(status) = \"firing\" ? \"problem\": \"resolution\",\n          \"classification\": labels.alertname\n        },\n        \"links\": [\n          {\n              \"url\": generatorURL\n          }\n        ],\n        \"sender\": {\n          \"name\": \"Prometheus\",\n          \"type\": \"Webhook Connector\"\n        },\n       \"details\": labels\n      }\n    )\n  )\n\nClick Save.\n\n\n\n\n3. Generate the Webhook and Capture the URL\n\nThe webhook will start initializing, wait as it intializes.\nA unique Webhook route will be displayed (e.g., https://&lt;aiops-domain&gt;/webhook-connector/&lt;id&gt;) once the webhook is Running.\nCopy this URL ‚Äî it will be used in the AlertmanagerConfig in Prometheus to send alerts to AIOps.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/configuration.html#prometheus-alertmanager-webhook-receiver-configuration-for-aiops",
    "href": "src/solution_overview/configuration.html#prometheus-alertmanager-webhook-receiver-configuration-for-aiops",
    "title": "AIOps on Linux Configuration",
    "section": "Prometheus Alertmanager: Webhook Receiver Configuration for AIOps",
    "text": "Prometheus Alertmanager: Webhook Receiver Configuration for AIOps\nThis section outlines the steps required to configure the Prometheus Operator‚Äôs Alertmanager to successfully send alerts to the AIOps webhook endpoint.\nThe process involves two main phases:\n\nNetwork Configuration: Ensuring the webhook FQDN is resolvable within the cluster.\nAlerting Configuration: Defining the Alertmanager receiver and routing.\n\n\n\n1. Network Configuration (DNS Resolution)\nThe Alertmanager pod must be able to resolve the AIOps webhook FQDN (e.g.¬†whconn-d59baea5-a620-4efd-bfdc-bbbce5314530-aiops.aiops-haproxy.gym.lan). Since this FQDN is custom and resolves to a specific HAProxy IP (192.168.252.9), the entry must be added to CoreDNS.\n\nUpdate the coredns-custom ConfigMap\nEdit the coredns-custom ConfigMap in the kube-system namespace to include the webhook domain, mapping it to your HAProxy IP (192.168.252.9). This approach is necessary since standard Kubernetes DNS does not resolve external domains.\nNote: Replace 192.168.252.9 with your actual HAProxy IP if different. Replace &lt;webhook route&gt; with the fqdn from the webhook route generated by AIOps (e.g.¬†whconn-d59baea5-a620-4efd-bfdc-bbbce5314530-aiops.aiops-haproxy.gym.lan)\nAdditional Note: The below ConfigMap also contains additional DNS mapping to the CloudPak console and the AIOPs UI. This may or may not be applicable to your environment.\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns-custom\n  namespace: kube-system\napiVersion: v1\ndata:\n  default.server: |\n    cp-console-aiops.aiops-haproxy.gym.lan {\n        hosts {\n              192.168.252.9 cp-console-aiops.aiops-haproxy.gym.lan\n              fallthrough\n        }\n    }\n    aiops-cpd.aiops-haproxy.gym.lan {\n        hosts {\n              192.168.252.9 aiops-cpd.aiops-haproxy.gym.lan\n              fallthrough\n        }\n    }\n    &lt;webhook route&gt; {\n        hosts {\n              192.168.252.9 &lt;webhook route&gt;\n              fallthrough\n        }\n    }\nEOF\n\n\nRestart CoreDNS\nForce CoreDNS to reload the new ConfigMap by restarting the deployment:\nkubectl -n kube-system rollout restart deployment coredns\n\nAfter CoreDNS restarts, the Alertmanager will be able to resolve the hostname, and all firing alerts will successfully flow to your AIOps webhook.\n\n\n\n\n2. Configure Alertmanager Receiver\nSince the Prometheus Operator uses the AlertmanagerConfig Custom Resource (CRD), we define the webhook receiver and routing within this resource.\n\nDefine the AlertmanagerConfig CR\nCreate or update the AlertmanagerConfig CR (named aiops-webhook-receiver in this example) to include the receiver and routing.\nReplace the sample webhook route https://whconn-d59baea5-a620-4efd-bfdc-bbbce5314530-aiops.aiops-haproxy.gym.lan/webhook-connector/fj3u0bq23tk with your actual webhook route and save to a file named aiops-alertmanagerconfig.yaml.\napiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: aiops-webhook-receiver\n  namespace: prometheus-operator # Must be in the same namespace as Alertmanager\n  labels:\n    alertmanagerConfig: main # Must match your Alertmanager CR selector\nspec:\n  # 1. Define the Receiver\n  receivers:\n  - name: 'aiops-webhook-receiver'\n    webhookConfigs:\n      - url: 'https://whconn-d59baea5-a620-4efd-bfdc-bbbce5314530-aiops.aiops-haproxy.gym.lan/webhook-connector/fj3u0bq23tk' # REPLACE\n        sendResolved: true\n        # Required for self-signed certificates\n        httpConfig:\n          tlsConfig:\n            insecureSkipVerify: true\n          \n  # 2. Define the Route\n  route:\n    receiver: 'aiops-webhook-receiver' # Route all alerts to the new receiver\n    groupBy: ['alertname', 'severity'] \n    groupWait: 30s\n    groupInterval: 5m\n    repeatInterval: 4h\n\n\nApply the Configuration\nApply the manifest:\nkubectl apply -f aiops-alertmanagerconfig.yaml\n\n\n\n\n3. Alert Lifecycle\nThis section assumes that you have created a rule in Prometheus to trigger an alert if an AIOps node root filesystem / usage exceeds 90%.\n\nTrigger Storage Alert\nUse the following script trigger_disk_alert.sh to trigger a storage alert on the root fileystem of an AIOps node.\n#!/bin/bash\n\n# Configuration\nTARGET_PERCENT=90\nMOUNT_POINT=\"/\"\nSAFETY_BUFFER_MB=10 # Add 10MB buffer to ensure we pass the threshold\nTARGET_FILE=\"/tmp/ROOT_FILL_FILE.bin\"\n\necho \"--- Disk Usage Alert Trigger ---\"\n\n# 1. Get disk statistics for the root filesystem in Kilobytes (KB)\n# Uses df -k to get output in KB for precise calculation\nif ! STATS=$(df -k \"${MOUNT_POINT}\" 2&gt;/dev/null | awk 'NR==2{print $2, $3}'); then\n    echo \"Error: Failed to get disk statistics for ${MOUNT_POINT}. Exiting.\"\n    exit 1\nfi\n\nTOTAL_KB=$(echo \"$STATS\" | awk '{print $1}')\nUSED_KB=$(echo \"$STATS\" | awk '{print $2}')\n# AVAILABLE_KB is not strictly needed for the calculation, but useful for debugging\n\n# Calculate percentages using integer arithmetic (multiplying by 100 first for precision)\nCURRENT_PERCENT=$(( (USED_KB * 100) / TOTAL_KB ))\n\n# Convert KB to MB for display purposes only\nTOTAL_MB=$(( TOTAL_KB / 1024 ))\nUSED_MB=$(( USED_KB / 1024 ))\n\necho \"Filesystem: ${MOUNT_POINT}\"\necho \"Total Size: ${TOTAL_MB} MB\"\necho \"Used Size:  ${USED_MB} MB (${CURRENT_PERCENT}%)\"\necho \"Target:     ${TARGET_PERCENT}% usage\"\n\n# 2. Check if the disk is already above the target\n# Integer check: If (Current Used KB * 100) is &gt;= (Total KB * Target Percent)\nif [ $(( USED_KB * 100 )) -ge $(( TOTAL_KB * TARGET_PERCENT )) ]; then\n    echo \"Current usage (${CURRENT_PERCENT}%) is already above the target (${TARGET_PERCENT}%). No file created.\"\n    exit 0\nfi\n\n# 3. Calculate the required KB to reach the target percentage\n# T_target_KB = (TOTAL_KB * TARGET_PERCENT) / 100\nTARGET_USAGE_KB=$(( (TOTAL_KB * TARGET_PERCENT) / 100 ))\n\n# Calculate buffer size in KB\nSAFETY_BUFFER_KB=$(( SAFETY_BUFFER_MB * 1024 ))\n\n# Required KB = (Target KB - Current Used KB) + Safety Buffer KB\nREQUIRED_KB=$(( TARGET_USAGE_KB - USED_KB + SAFETY_BUFFER_KB ))\n\n\n# 4. Convert required KB to MB (dd count uses 1MB blocks) and round up\n# Use shell arithmetic for simple rounding up: (KB + 1023) / 1024\nREQUIRED_MB_COUNT=$(( (REQUIRED_KB + 1023) / 1024 ))\n\n# 5. Execute dd command\necho \"--------------------------------------\"\necho \"Creating file of size: ${REQUIRED_MB_COUNT} MB at ${TARGET_FILE}\"\necho \"This will push usage over ${TARGET_PERCENT}%...\"\n\n# Execute the dd command using the calculated count\n# Note: Requires sudo access to write to the filesystem\nsudo dd if=/dev/zero of=\"${TARGET_FILE}\" bs=1M count=\"${REQUIRED_MB_COUNT}\" 2&gt;/dev/null\n\n# 6. Final verification (Use awk to extract the percentage from df -h)\nNEW_PERCENT=$(df -h \"${MOUNT_POINT}\" | awk 'NR==2{print $5}')\necho \"Creation complete.\"\necho \"New usage: ${NEW_PERCENT}\"\necho \"--------------------------------------\"\n\nexit 0\nRun the script.\nchmod +x trigger_disk_alert.sh && ./trigger_disk_alert.sh\nSample output.\n--- Disk Usage Alert Trigger ---\nFilesystem: /\nTotal Size: 2916 MB\nUsed Size:  2041 MB (69%)\nTarget:     90% usage\n--------------------------------------\nCreating file of size: 594 MB at /tmp/ROOT_FILL_FILE.bin\nThis will push usage over 90%...\nCreation complete.\nNew usage: 91%\n--------------------------------------\n\n\nAlert in Prometheus\nLog in to Prometheus Explorer Alerts console with your AIOps credentials. The URL is https://aiops-cpd.&lt;domain&gt;/self-monitoring/explorer/alerts where &lt;domain&gt; is the network domain AIOps is installed on (e.g.¬†https://aiops-cpd.aiops-haproxy.gym.lan/self-monitoring/explorer/alerts).\nWithin a few minutes you will see a NodeDiskUsage alert firing.\n\n\n\nAlert in AIOps\nIn AIOps, navigate to the Alerts list. Here you will see the critical Prometheus alert for High Disk Usage.\n\nDouble click on the alert to open the details.\n\n\n\nResolve Alert\nOn the same not where you triggered the disk usage script. Resolve the disk consumption by deleting the created file.\nsudo rm -f /tmp/ROOT_FILL_FILE.bin\nAfter a few minutes, Prometheus will clear the alert and the clear action will cascade to AIOps.",
    "crumbs": [
      "Solution Overview",
      "Configure"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html",
    "href": "src/solution_overview/administration.html",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "This section outlines the procedure for safely performing maintenance (e.g., operating system patching, security updates, or reboots) on K3s cluster nodes that are part of AIOps on Linux. This is a rolling process, meaning only one node is taken offline at a time to maintain service continuity.\n\n\n\nAccess: SSH access to the node(s) to be rebooted.\nPermissions: Kubernetes configuration file (~/.kube/config or root access) for running kubectl commands.\nScript: The k3s-capacity-check.sh script to analyze resource availability.\n\n\n\n\n\nBefore starting any maintenance, you must ensure the remaining nodes have enough guaranteed resources (Memory Requests) to absorb the workload of the node you plan to drain.\n\n\nYou have two options to obtain the script: download it directly from the repository or copy/paste it via the console.\n\n\n\nDownload: Save the script to a server/control-plane node: k3s-capacity-check\nMake Executable: Set the execution permission on the script file:\nchmod +x k3s-capacity-check.sh\n\n\n\n\nClick to expand the section below to view the full script logic. Copy and paste all the contents into a new file named k3s-capacity-check on your target system, then follow step 2 above to make it executable.\n\n\nView Script Source: k3s-capacity-check.sh\n\n#!/bin/bash\n\n# Check if the script is run as root (UID 0)\nif [[ $UID -eq 0 ]]; then\n    # Running as root: use the default kubectl command, assuming necessary environment/configs are set for root.\n    KUBECTL_CMD=\"kubectl\"\n    echo \"Running as root. Using standard 'kubectl'.\"\nelse\n    # Running as non-root: explicitly specify the kubeconfig file.\n    # We use $HOME for the home directory of the current user.\n    KUBECTL_CMD=\"kubectl --kubeconfig $HOME/.kube/config\"\n    echo \"Running as non-root. Using '${KUBECTL_CMD}'.\"\nfi\n\n# Resource to check for the critical bottleneck (Memory is typically the most critical)\nRESOURCE=\"memory\"\n# The scaling factor for Ki to Mi (1024)\nSCALE_FACTOR=1024\n\n# --- Helper Functions (Using AWK for Division) ---\n\n# Function to safely convert any unit (Ki/Mi/Raw Bytes) to MiB (Megabytes)\n# $1: Value string from kubectl (e.g., \"26874661376\", \"29700Mi\", \"38126949Ki\")\nconvert_to_mib() {\n    local val=$1\n    local unit=$(echo \"$val\" | grep -oE '[a-zA-Z]+$')\n    local num=$(echo \"$val\" | grep -oE '^[0-9]+')\n\n    if [[ -z \"$num\" ]]; then\n        echo 0\n        return\n    fi\n\n    # Use awk to handle floating point conversion and rounding\n    if [[ \"$unit\" == \"Ki\" ]]; then\n        # Convert Ki to Mi: Ki / 1024\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1024}'\n    elif [[ \"$unit\" == \"Mi\" ]]; then\n        # Value is already Mi, just echo it\n        echo \"$num\"\n    else\n        # Assume raw bytes if no unit is found, convert Bytes to Mi: Bytes / (1024 * 1024)\n        # Note: We must be cautious with very large byte numbers in awk on some systems.\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1048576}'\n    fi\n}\n\n# --- Data Collection and Calculation ---\n\n# Get a list of all schedulable worker and server nodes\nNODES=$($KUBECTL_CMD get nodes --no-headers -o custom-columns=NAME:.metadata.name)\nNUM_NODES=$(echo \"$NODES\" | wc -l)\n\nTOTAL_CAPACITY_MI=0\nTOTAL_REQUESTS_MI=0\nMAX_NODE_REQUESTS_MI=0\nBUSIEST_NODE=\"\"\n\necho \"--- Kubernetes Cluster Capacity Analysis ---\"\necho \"Using command: ${KUBECTL_CMD}\"\necho \"Analyzing ${NUM_NODES} nodes. Critical Resource: ${RESOURCE^}.\"\necho \"------------------------------------------------\"\n\nfor NODE in $NODES; do\n    # 1. Get Node Capacity (Allocatable)\n    # Extracts the Allocatable memory value (e.g., \"34000Mi\" or \"35000000Ki\")\n    CAPACITY_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk \"/^Allocatable:/{flag=1; next} /${RESOURCE}/ && flag{print \\$2; exit}\" | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    CAPACITY_MI=$(convert_to_mib \"$CAPACITY_VAL\")\n\n    # 2. Get Node Requests\n    # Extracts the Requested memory value (e.g., \"26874661376\", \"29700Mi\")\n    # This AWK command is now carefully structured to grab the *second* field for memory in the \"Allocated resources\" block\n    REQUESTS_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk '/Allocated resources:/,/Events:/{if ($1 == \"memory\") print $2; if ($1 == \"cpu\") print $2}' | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    REQUESTS_MI=$(convert_to_mib \"$REQUESTS_VAL\")\n\n    # Handle cases where request data is missing or failed conversion\n    if [ -z \"$CAPACITY_MI\" ] || [ -z \"$REQUESTS_MI\" ]; then\n        echo \"Warning: Skipped $NODE due to missing data or conversion error.\" &gt;&2\n        continue\n    fi\n\n    # 3. Calculate Totals and Busiest Node (Bash Integer Math)\n    TOTAL_CAPACITY_MI=$((TOTAL_CAPACITY_MI + CAPACITY_MI))\n    TOTAL_REQUESTS_MI=$((TOTAL_REQUESTS_MI + REQUESTS_MI))\n\n    if [ \"$REQUESTS_MI\" -gt \"$MAX_NODE_REQUESTS_MI\" ]; then\n        MAX_NODE_REQUESTS_MI=\"$REQUESTS_MI\"\n        BUSIEST_NODE=\"$NODE\"\n    fi\ndone\n\n# --- Final Calculations and Summary Output (Bash Integer Math) ---\nFREE_CAPACITY_MI=$((TOTAL_CAPACITY_MI - TOTAL_REQUESTS_MI))\nNET_CAPACITY_AFTER_DRAIN=$((FREE_CAPACITY_MI - MAX_NODE_REQUESTS_MI))\n\necho \"------------------------------------------------\"\necho \"--- Cluster Totals (for ${RESOURCE^}) ---\"\necho \"Total Cluster Allocatable: $((TOTAL_CAPACITY_MI / 1024)) Gi\"\necho \"Total Cluster Requests:    $((TOTAL_REQUESTS_MI / 1024)) Gi\"\necho \"Total Cluster Free Capacity: $((FREE_CAPACITY_MI / 1024)) Gi\"\necho \"------------------------------------------------\"\necho \"--- Maintenance Prediction ---\"\n\nif [ \"$NET_CAPACITY_AFTER_DRAIN\" -ge 0 ]; then\n    echo \"‚úÖ PREDICTION: SUCCESSFUL\"\n    echo \"The cluster has enough guaranteed capacity (Memory) to absorb the busiest node's workload.\"\n    echo \"Remaining Free Capacity after draining busiest node: $((NET_CAPACITY_AFTER_DRAIN / 1024)) Gi\"\nelse\n    echo \"üö® PREDICTION: FAILURE RISK\"\n    echo \"The cluster does NOT have enough guaranteed free capacity (Memory) to absorb the busiest node's workload.\"\n    # Use integer math for the negative result and division\n    CAPACITY_SHORTFALL=$((-NET_CAPACITY_AFTER_DRAIN))\n    echo \"Capacity Shortfall: $((CAPACITY_SHORTFALL / 1024)) Gi\"\nfi\n\necho \"------------------------------------------------\"\necho \"HIGHEST RISK NODE (If drained): $BUSIEST_NODE\"\necho \"Load to be re-scheduled: $((MAX_NODE_REQUESTS_MI / 1024)) Gi\"\necho \"\"\n\n\n\n\n\nExecute the script to determine if you have enough capacity for a safe drain:\n./k3s-capacity-check.sh\n\nAnalyze the Prediction:\n\n‚úÖ SUCCESSFUL: The Remaining Free Capacity is positive. Proceed to Step 2.\nüö® FAILURE RISK: The script reports a Capacity Shortfall. DO NOT PROCEED. Scale up your cluster until the script predicts success.\n\n\n\n\n\n\n\nThis step removes the node‚Äôs workload and prevents new Pods from being scheduled to it.\n\nIdentify the Target Node: Choose the safest node first (the one with the lowest ‚ÄúLoad to be re-scheduled‚Äù as reported by the script) or the first node in your rotation.\nCordon and Drain the Node: Run the kubectl drain command. This automatically marks the node as Unschedulable (Cordon) and safely evicts all running Pods.\n# Replace &lt;node-name&gt; with the actual node name, e.g., aiops-k3s-agent-0.gym.lan\nkubectl drain &lt;node-name&gt; \\\n  --ignore-daemonsets \\\n  --delete-emptydir-data\n\n\n\n\n\n\n\nFlag\nPurpose\n\n\n\n\n--ignore-daemonsets\nEnsures critical cluster services (managed by DaemonSets) are not evicted.\n\n\n--delete-emptydir-data\nRequired to evict Pods using EmptyDir volumes (which are temporary and local).\n\n\n\nVerify the Drain: Confirm the node status and that no user Pods remain.\nkubectl get nodes\n# Status should show \"&lt;node-name&gt; Ready,SchedulingDisabled\"\n\n\n\n\n\nOnce the node is drained, it is safe to perform the necessary operating system work.\n\nSSH and Update: SSH into the drained node and perform OS patching/updates.\nssh &lt;node-name&gt;\n# e.g., sudo apt update && sudo apt upgrade -y\nReboot: Reboot the server to finalize updates.\nsudo reboot\nWait for Ready State: Wait until the node reboots and its status changes back to Ready (but still SchedulingDisabled). This may take a few minutes as K3s/kubelet starts up.\n# Run this periodically from another node\nkubectl get nodes\n\n\n\n\n\n\nUncordon the Node: Mark the node as Schedulable so the Kubernetes scheduler can once again place Pods onto it.\nkubectl uncordon &lt;node-name&gt;\nVerify Pod Rescheduling: Check that the Pods evicted during the drain process have successfully rescheduled themselves across the cluster, including back onto the newly available node.\nVerify Cluster Health: Check the health of the entire cluster before moving on to the next node.\nkubectl get pods -A | grep -v 'Running'\n# Ensure no Pods are stuck in a Pending or CrashLoopBackOff state.\n\nRepeat Steps 2 through 4 for the next node in the maintenance cycle. Always re-verify capacity if there have been significant workload changes.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#prerequisites",
    "href": "src/solution_overview/administration.html#prerequisites",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Access: SSH access to the node(s) to be rebooted.\nPermissions: Kubernetes configuration file (~/.kube/config or root access) for running kubectl commands.\nScript: The k3s-capacity-check.sh script to analyze resource availability.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-1-verify-cluster-capacity-before-starting",
    "href": "src/solution_overview/administration.html#step-1-verify-cluster-capacity-before-starting",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Before starting any maintenance, you must ensure the remaining nodes have enough guaranteed resources (Memory Requests) to absorb the workload of the node you plan to drain.\n\n\nYou have two options to obtain the script: download it directly from the repository or copy/paste it via the console.\n\n\n\nDownload: Save the script to a server/control-plane node: k3s-capacity-check\nMake Executable: Set the execution permission on the script file:\nchmod +x k3s-capacity-check.sh\n\n\n\n\nClick to expand the section below to view the full script logic. Copy and paste all the contents into a new file named k3s-capacity-check on your target system, then follow step 2 above to make it executable.\n\n\nView Script Source: k3s-capacity-check.sh\n\n#!/bin/bash\n\n# Check if the script is run as root (UID 0)\nif [[ $UID -eq 0 ]]; then\n    # Running as root: use the default kubectl command, assuming necessary environment/configs are set for root.\n    KUBECTL_CMD=\"kubectl\"\n    echo \"Running as root. Using standard 'kubectl'.\"\nelse\n    # Running as non-root: explicitly specify the kubeconfig file.\n    # We use $HOME for the home directory of the current user.\n    KUBECTL_CMD=\"kubectl --kubeconfig $HOME/.kube/config\"\n    echo \"Running as non-root. Using '${KUBECTL_CMD}'.\"\nfi\n\n# Resource to check for the critical bottleneck (Memory is typically the most critical)\nRESOURCE=\"memory\"\n# The scaling factor for Ki to Mi (1024)\nSCALE_FACTOR=1024\n\n# --- Helper Functions (Using AWK for Division) ---\n\n# Function to safely convert any unit (Ki/Mi/Raw Bytes) to MiB (Megabytes)\n# $1: Value string from kubectl (e.g., \"26874661376\", \"29700Mi\", \"38126949Ki\")\nconvert_to_mib() {\n    local val=$1\n    local unit=$(echo \"$val\" | grep -oE '[a-zA-Z]+$')\n    local num=$(echo \"$val\" | grep -oE '^[0-9]+')\n\n    if [[ -z \"$num\" ]]; then\n        echo 0\n        return\n    fi\n\n    # Use awk to handle floating point conversion and rounding\n    if [[ \"$unit\" == \"Ki\" ]]; then\n        # Convert Ki to Mi: Ki / 1024\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1024}'\n    elif [[ \"$unit\" == \"Mi\" ]]; then\n        # Value is already Mi, just echo it\n        echo \"$num\"\n    else\n        # Assume raw bytes if no unit is found, convert Bytes to Mi: Bytes / (1024 * 1024)\n        # Note: We must be cautious with very large byte numbers in awk on some systems.\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1048576}'\n    fi\n}\n\n# --- Data Collection and Calculation ---\n\n# Get a list of all schedulable worker and server nodes\nNODES=$($KUBECTL_CMD get nodes --no-headers -o custom-columns=NAME:.metadata.name)\nNUM_NODES=$(echo \"$NODES\" | wc -l)\n\nTOTAL_CAPACITY_MI=0\nTOTAL_REQUESTS_MI=0\nMAX_NODE_REQUESTS_MI=0\nBUSIEST_NODE=\"\"\n\necho \"--- Kubernetes Cluster Capacity Analysis ---\"\necho \"Using command: ${KUBECTL_CMD}\"\necho \"Analyzing ${NUM_NODES} nodes. Critical Resource: ${RESOURCE^}.\"\necho \"------------------------------------------------\"\n\nfor NODE in $NODES; do\n    # 1. Get Node Capacity (Allocatable)\n    # Extracts the Allocatable memory value (e.g., \"34000Mi\" or \"35000000Ki\")\n    CAPACITY_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk \"/^Allocatable:/{flag=1; next} /${RESOURCE}/ && flag{print \\$2; exit}\" | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    CAPACITY_MI=$(convert_to_mib \"$CAPACITY_VAL\")\n\n    # 2. Get Node Requests\n    # Extracts the Requested memory value (e.g., \"26874661376\", \"29700Mi\")\n    # This AWK command is now carefully structured to grab the *second* field for memory in the \"Allocated resources\" block\n    REQUESTS_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk '/Allocated resources:/,/Events:/{if ($1 == \"memory\") print $2; if ($1 == \"cpu\") print $2}' | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    REQUESTS_MI=$(convert_to_mib \"$REQUESTS_VAL\")\n\n    # Handle cases where request data is missing or failed conversion\n    if [ -z \"$CAPACITY_MI\" ] || [ -z \"$REQUESTS_MI\" ]; then\n        echo \"Warning: Skipped $NODE due to missing data or conversion error.\" &gt;&2\n        continue\n    fi\n\n    # 3. Calculate Totals and Busiest Node (Bash Integer Math)\n    TOTAL_CAPACITY_MI=$((TOTAL_CAPACITY_MI + CAPACITY_MI))\n    TOTAL_REQUESTS_MI=$((TOTAL_REQUESTS_MI + REQUESTS_MI))\n\n    if [ \"$REQUESTS_MI\" -gt \"$MAX_NODE_REQUESTS_MI\" ]; then\n        MAX_NODE_REQUESTS_MI=\"$REQUESTS_MI\"\n        BUSIEST_NODE=\"$NODE\"\n    fi\ndone\n\n# --- Final Calculations and Summary Output (Bash Integer Math) ---\nFREE_CAPACITY_MI=$((TOTAL_CAPACITY_MI - TOTAL_REQUESTS_MI))\nNET_CAPACITY_AFTER_DRAIN=$((FREE_CAPACITY_MI - MAX_NODE_REQUESTS_MI))\n\necho \"------------------------------------------------\"\necho \"--- Cluster Totals (for ${RESOURCE^}) ---\"\necho \"Total Cluster Allocatable: $((TOTAL_CAPACITY_MI / 1024)) Gi\"\necho \"Total Cluster Requests:    $((TOTAL_REQUESTS_MI / 1024)) Gi\"\necho \"Total Cluster Free Capacity: $((FREE_CAPACITY_MI / 1024)) Gi\"\necho \"------------------------------------------------\"\necho \"--- Maintenance Prediction ---\"\n\nif [ \"$NET_CAPACITY_AFTER_DRAIN\" -ge 0 ]; then\n    echo \"‚úÖ PREDICTION: SUCCESSFUL\"\n    echo \"The cluster has enough guaranteed capacity (Memory) to absorb the busiest node's workload.\"\n    echo \"Remaining Free Capacity after draining busiest node: $((NET_CAPACITY_AFTER_DRAIN / 1024)) Gi\"\nelse\n    echo \"üö® PREDICTION: FAILURE RISK\"\n    echo \"The cluster does NOT have enough guaranteed free capacity (Memory) to absorb the busiest node's workload.\"\n    # Use integer math for the negative result and division\n    CAPACITY_SHORTFALL=$((-NET_CAPACITY_AFTER_DRAIN))\n    echo \"Capacity Shortfall: $((CAPACITY_SHORTFALL / 1024)) Gi\"\nfi\n\necho \"------------------------------------------------\"\necho \"HIGHEST RISK NODE (If drained): $BUSIEST_NODE\"\necho \"Load to be re-scheduled: $((MAX_NODE_REQUESTS_MI / 1024)) Gi\"\necho \"\"\n\n\n\n\n\nExecute the script to determine if you have enough capacity for a safe drain:\n./k3s-capacity-check.sh\n\nAnalyze the Prediction:\n\n‚úÖ SUCCESSFUL: The Remaining Free Capacity is positive. Proceed to Step 2.\nüö® FAILURE RISK: The script reports a Capacity Shortfall. DO NOT PROCEED. Scale up your cluster until the script predicts success.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-2-prepare-the-node-for-maintenance-cordon-and-drain",
    "href": "src/solution_overview/administration.html#step-2-prepare-the-node-for-maintenance-cordon-and-drain",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "This step removes the node‚Äôs workload and prevents new Pods from being scheduled to it.\n\nIdentify the Target Node: Choose the safest node first (the one with the lowest ‚ÄúLoad to be re-scheduled‚Äù as reported by the script) or the first node in your rotation.\nCordon and Drain the Node: Run the kubectl drain command. This automatically marks the node as Unschedulable (Cordon) and safely evicts all running Pods.\n# Replace &lt;node-name&gt; with the actual node name, e.g., aiops-k3s-agent-0.gym.lan\nkubectl drain &lt;node-name&gt; \\\n  --ignore-daemonsets \\\n  --delete-emptydir-data\n\n\n\n\n\n\n\nFlag\nPurpose\n\n\n\n\n--ignore-daemonsets\nEnsures critical cluster services (managed by DaemonSets) are not evicted.\n\n\n--delete-emptydir-data\nRequired to evict Pods using EmptyDir volumes (which are temporary and local).\n\n\n\nVerify the Drain: Confirm the node status and that no user Pods remain.\nkubectl get nodes\n# Status should show \"&lt;node-name&gt; Ready,SchedulingDisabled\"",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-3-perform-maintenance-patch-and-reboot",
    "href": "src/solution_overview/administration.html#step-3-perform-maintenance-patch-and-reboot",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Once the node is drained, it is safe to perform the necessary operating system work.\n\nSSH and Update: SSH into the drained node and perform OS patching/updates.\nssh &lt;node-name&gt;\n# e.g., sudo apt update && sudo apt upgrade -y\nReboot: Reboot the server to finalize updates.\nsudo reboot\nWait for Ready State: Wait until the node reboots and its status changes back to Ready (but still SchedulingDisabled). This may take a few minutes as K3s/kubelet starts up.\n# Run this periodically from another node\nkubectl get nodes",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-4-complete-the-cycle-uncordon-and-verify",
    "href": "src/solution_overview/administration.html#step-4-complete-the-cycle-uncordon-and-verify",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Uncordon the Node: Mark the node as Schedulable so the Kubernetes scheduler can once again place Pods onto it.\nkubectl uncordon &lt;node-name&gt;\nVerify Pod Rescheduling: Check that the Pods evicted during the drain process have successfully rescheduled themselves across the cluster, including back onto the newly available node.\nVerify Cluster Health: Check the health of the entire cluster before moving on to the next node.\nkubectl get pods -A | grep -v 'Running'\n# Ensure no Pods are stuck in a Pending or CrashLoopBackOff state.\n\nRepeat Steps 2 through 4 for the next node in the maintenance cycle. Always re-verify capacity if there have been significant workload changes.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIOps Handbook",
    "section": "",
    "text": "MD",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#description-aiops-handbook",
    "href": "index.html#description-aiops-handbook",
    "title": "AIOps Handbook",
    "section": "Description: ‚ÄúAIOps Handbook‚Äù",
    "text": "Description: ‚ÄúAIOps Handbook‚Äù",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "AIOps Handbook",
    "section": "The Why",
    "text": "The Why\n(High-level description of the business challenges and client pain points)\n\nProblem Details\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.\n\n\nAdditional Context\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "AIOps on Linux Planning",
    "section": "",
    "text": "Networking\n\nCan you use an alternative port to 443 for the load balancer?\nNo. As of May 2025, this does not work and port 443 must be used as the UI port for the load balancer. There are AIOps services within the cluster that will attempt to access cp-console-aiops via port 443.\n\n\n\nNon-root Users\nThe installation of AIOps on Linux requires root (or sudo) and the underlying components run as root on the nodes. However, you can configure your nodes such that a non-root user can do most types of administrative tasks after the installation.\nThe commands below can be run as root after cluster node up is successful on the control plane (k3s server) nodes to configure a group called k3sadmin that will allow a non-root user called clouduser to run administrative commands.\ngroupadd k3sadmin\nusermod -aG k3sadmin clouduser\n\nchown root:k3sadmin /usr/local/bin/k3s\nchmod 750 /usr/local/bin/k3s\n\nchown root:k3sadmin /etc/rancher/\nchmod 750 /etc/rancher/\n\nchown -R root:k3sadmin /etc/rancher/k3s/\nchmod 750 /etc/rancher/k3s/\n\nchmod 750 /etc/rancher/k3s/config.yaml\nchmod 660 /etc/rancher/k3s/k3s.yaml\n\n# for ctr\nchown root:k3sadmin /var/lib/rancher/k3s/agent/etc/\nchmod 750 /var/lib/rancher/k3s/agent/etc/\n# for crictl\nchown root:k3sadmin /var/lib/rancher/k3s/agent/etc/crictl.yaml\nchmod 640 /var/lib/rancher/k3s/agent/etc/crictl.yaml\n\n# ONLY for online installs, oc is not available for offline installs\n# for oc\nmkdir -p /home/clouduser/.kube\ncp /etc/rancher/k3s/k3s.yaml /home/clouduser/.kube/config\nchown -R clouduser:clouduser /home/clouduser/.kube\nAfter executing these commands, clouduser will be able to run kubectl, crictl, ctr, aiopsctl, and oc (if online install is used).\n\n‚ùì Where is oc in my offline install?\nThe oc binary is downloaded from openshift.com during installation. For an offline install it assumed that access to openshift.com is not present. But don‚Äôt worry, you can use kubectl instead. :-)\n\n\n\nOffline Installation\nOffline installations, also known as air-gapped deployment, is necessary if the target cluster will not have access to the internet.\n\n‚ùó Security scanning for container images ‚ùó\nIBM does internal security scanning of container images but it is very common for enterprises to require their own security scans for container images prior to allowing installation. THIS CAN BE A VERY LENGTHY PROCESS. Cloud Pak for AIOPs publishes over 400 different container images and every enterprise handles this situation differently.\nIf this is a requirement for your enterprise, get started on this process first as it may quickly become a blocker for the deployment schedule.\n\n‚ùì Where do I find the list of container images ‚ùì\nThe container images for Cloud Pak for AIOps can vary depending on release. There is no way to have the aiopsctl command generate list without also attempting to mirror the images. Here is a hack that will kill the process after it generates the proper manifest file but before the mirroring starts (change the --registry parameter value to match your environment):\n# run aiopsctl bastion login commands for both cp.icr.io and the target registry first\naiopsctl bastion mirror-images --registry artifactory.gym.lan:8443/docker-local 2&gt;&1 | tee &gt;(\n    while read line; do\n        if [[ \"$line\" == *'Mirroring images to registry'* ]]; then\n            echo \"String found. Killing process...\"\n            sleep 5 # time for oc to install\n            pkill -P $$ aiopsctl\n            exit 0\n        fi\n    done\n)\nThis will generate a file called images-mapping.txt under ~/.aiopsctl. The directory will differ depending on the release, but for v4.10.1 the location is ~/.aiopsctl/mirror/.ibm-pak/data/mirror/ibm-cp-waiops/1.16.1\nExample of images-mapping.txt (only first 4 lines shown):\ncp.icr.io/cp/appc/ace-server-prod@sha256:2f9e903bd2cf53ba0878e982ac0f81d956200ffe70a9a57084c2b31bf9134311=artifactory.gym.lan:8443/docker-local/cp/appc/ace-server-prod:13.0.2.2-r2-20250315-121329\ncp.icr.io/cp/appc/acecc-content-server-prod@sha256:4a558be37f530acc1c2395187bbe6d896a2f8dca3c2c9977eb66b7e984805abe=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-content-server-prod:13.0.2.2-r2-20250318-034910\ncp.icr.io/cp/appc/acecc-couchdb3-prod@sha256:c18d850494ba2ee8a90859430e4cd7ea2e6c342eea750afc816f450520dd42c8=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-couchdb3-prod:13.0.2.2-r2-20250318-020015\ncp.icr.io/cp/appc/acecc-dashboard-prod@sha256:ab27d8031bb8ec87a3e465c4e172d9c57cd0b415db78d8305e45f74820949d58=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-dashboard-prod:13.0.2.2-r2-20250325-140612\nFor each line, the section before = is the source container image from IBM container registry.\nTo list only the source images from the images-mapping.txt file, use the following command:\nawk -F= '{print $1}' images-mapping.txt\n\n\n‚ùì Do I really need all of these container images ‚ùì\nMaybe. The container images that make up a release contain back-level images for upgrade purposes. If you are installing AIOps for the first time, you do not need the full set of container images. This is an important detail as security scans might pick up vulnerabilities on container images that are not used in the current version of the product.\nFor example, AIOps might use kafka v3.8.0 and v3.9.0. However, for upgrade purposes kafka v3.6.1 is also be part of the container image manifest. If a security scan finds vulnerabilities in the kafka v3.6.1 image, those can be safely ignore for a new installation as they won‚Äôt be used.\n\n\n‚ùì How do I reduce the image manifest ‚ùì\nWARNING: Be very careful and only attempt this if you know what are you doing. You risk not including a required container image and the below has only been tested on v4.10.1.\nGo to the directoy containing the images-mapping.txt file generated from the bastion mirror-images command. The following command will create a new file called images-mapping-filtered.txt with most of the back-level container images removed.\nawk 'NR==FNR {split($0, a, \"@\"); split($0, b, \":\"); if (b[length(b)] ~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/) versioned_images[a[1]]++; next} {split($0, a, \"@\"); split($0, b, \":\"); if (!(a[1] in versioned_images && b[length(b)] !~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/)) print}' images-mapping.txt images-mapping.txt &gt; images-mapping-filtered.txt\nThere are a few different container types that need to be handled differently (redis, kafka). Run the command below to handle those.\nawk '\n    NR==FNR {\n        if ($0 ~ /ibm-redis-cp-haproxy@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version &gt; max_redis_haproxy_version) max_redis_haproxy_version = version;\n        } else if ($0 ~ /ibm-redis-cp-7.2.7@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version &gt; max_redis_version) max_redis_version = version;\n        } else if ($0 ~ /ibm-events-kafka-[0-9]+\\.[0-9]+\\.[0-9]+@/) {\n            match($0, /ibm-events-kafka-([0-9]+\\.[0-9]+\\.[0-9]+)@/, m);\n            if (m[1] &gt; max1_kafka_version) {\n                max2_kafka_version = max1_kafka_version; max1_kafka_version = m[1];\n            } else if (m[1] &gt; max2_kafka_version) {\n                max2_kafka_version = m[1];\n            }\n        } else {\n            split($0, a, \"@\");\n            split($0, b, \":\");\n            if (b[length(b)] ~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/) {\n                versioned_images[a[1]]++;\n            }\n        }\n        next;\n    }\n    {\n        if ($0 ~ /ibm-redis-cp-haproxy@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version == max_redis_haproxy_version) print;\n        } else if ($0 ~ /ibm-redis-cp-7.2.7@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version == max_redis_version) print;\n        } else if ($0 ~ /ibm-events-kafka-[0-9]+\\.[0-9]+\\.[0-9]+@/) {\n            match($0, /ibm-events-kafka-([0-9]+\\.[0-9]+\\.[0-9]+)@/, m);\n            if (m[1] == max1_kafka_version || m[1] == max2_kafka_version) print;\n        } else {\n            split($0, a, \"@\");\n            split($0, b, \":\");\n            if (!(a[1] in versioned_images && b[length(b)] !~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/)) {\n                print;\n            }\n        }\n    }\n' images-mapping-filtered.txt images-mapping-filtered.txt &gt; images-mapping-filtered.tmp && mv -f images-mapping-filtered.tmp images-mapping-filtered.txt\nYou now have images-mapping-filtered.txt that should be a significantly reduced set of images that will still work for a new installation.\n\n\n‚ùì How do I mirror my reduced images set ‚ùì\nWhen aiops bastion mirror-images runs it creates a set of files that do the mirroring under the covers. Those files are in .aiopsctl/mirror.\nGo to .aiopsctl/mirror and look at oc_image_mirror.sh.\n#!/bin/sh\nset -e\nset -o noglob\n\n# Usage\n#   ENV_VAR=... ./oc_image_mirror.sh &lt;Path to oc binary&gt;\n#\n# Environment variables:\n# - REGISTRY_AUTH_FILE\n#   Your registry authentication file. For Podman, this\n#   is typically `${XDG_RUNTIME_DIR}/containers/auth.json`.\n# - IMAGES_MAPPING_FILE\n#   Your images-mapping.txt to use as the source for mirroring.\n# - DRY_RUN\n#   Set to true to complete a dry-run mirror.\n#\n\n$1 image mirror \\\n-f \"${IMAGES_MAPPING_FILE}\" \\\n-a \"${REGISTRY_AUTH_FILE}\" \\\n--insecure \\\n--filter-by-os '.*' \\\n--skip-multiple-scopes \\\n--max-per-registry=1 \\\n--dry-run=\"${DRY_RUN}\"\nTip: If you want to speed up the mirroring, you can edit this file and change the --max-per-registry=1 to --max-per-registry=6. This will run 6 threads during the mirroring process.\nTo run the mirroring with our reduced images set images-mapping-filtered.txt, run the following command (use the proper path to your file if different).\n# change the path to your file if different\nIMAGES_MAPPING_FILE=./.ibm-pak/data/mirror/ibm-cp-waiops/1.16.1/images-mapping-filtered.txt REGISTRY_AUTH_FILE=./containers/auth.json DRY_RUN=false nohup ./oc_image_mirror.sh ./oc &gt;nohup.out 2&gt;&1 &lt;/dev/null &\nWhile the job is calculating the images to mirror, there will be no output for up to 10 minutes, but then you should see messages showing the mirroring progress in nohup.out. A fully reduced v4.10.1 container set with 6 threads should complete in less than 30 minutes.\n\n\n\nArtifactory\nIf planning for an offline installation using Artifactory as the target registry, ensure that Artifactory meets the requirements for AIOps.\n\nThe target repository layout must be set to Docker\nDocker V2 API must be enabled on the repository\n\n\nüß™ Docker repository validation\nLog in to the Artifactory UI and navigate to the target repository. There will be a type listed for the repository. That type should be Docker.\n\n\n\nüß™ Docker V2 API validation\nTest if the Docker v2 API is enabled by running:\ncurl -u &lt;username&gt;:&lt;password&gt; https://&lt;your-artifactory-domain&gt;/artifactory/&lt;repo-name&gt;/v2/\n\nIf it returns a 200 OK or 401 Unauthorized, the v2 API is enabled.\nIf it returns 404 Not Found, the repo may not support Docker v2.\n\n\n\n‚úÖ Artifactory URL Format\nWhen configuring your container image mirroring to Artifactory, follow these guidelines carefully to ensure compatibility and avoid common pitfalls.\nWhen specifying the Artifactory URL during the mirroring process:\n\n‚ùå Do not include https:// at the beginning.\n‚úÖ Do append :443 to the hostname.\n‚ùå Do not include /artifactory/ in the path like the file URL might display in the Artifactory UI. This is for file downloads only.\n‚úÖ Use the format: &lt;artifactory-host&gt;:443/&lt;repo&gt;/&lt;path-to-images&gt;\n\nFor example, if your Artifactory host is my-artifactory.company.com and your repository is internal-repo, the correct URL might be: my-artifactory.company.com:443/internal-repo/aiops\n\n\nüñ•Ô∏è Platform Requirements\nEnsure that you are running on RHEL 8 or RHEL 9 from the bastion host used to run the mirroring, as the oc version bundled with aiopsctl is compatible with RHEL 8 & 9.\nYou can verify your OS version with:\ncat /etc/redhat-release\nAnd check your oc version with:\noc version",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Solution"
  },
  {
    "objectID": "src/solution_overview/deploy.html",
    "href": "src/solution_overview/deploy.html",
    "title": "AIOps on Linux Deployment",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Solution Overview",
      "Deploy"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html",
    "href": "src/implementation_methodology/stepone-imp.html",
    "title": "Step One",
    "section": "",
    "text": "Step One Implementation - The How\n(Details the process and techniques used to execute the technical solution)"
  },
  {
    "objectID": "src/implementation_methodology/stepthree-imp.html",
    "href": "src/implementation_methodology/stepthree-imp.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  }
]