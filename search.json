[
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Best Practices\n(Capture the main takeaways and results of the project)"
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html",
    "href": "src/implementation_methodology/steptwo-imp.html",
    "title": "Step Two",
    "section": "",
    "text": "Step Two Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Solution"
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "AIOps on Linux Planning",
    "section": "",
    "text": "Networking\n\nCan you use an alternative port to 443 for the load balancer?\nNo. As of May 2025, this does not work and port 443 must be used as the UI port for the load balancer. There are AIOps services within the cluster that will attempt to access cp-console-aiops via port 443.\n\n\n\nNon-root Users\nThe installation of AIOps on Linux requires root (or sudo) and the underlying components run as root on the nodes. However, you can configure your nodes such that a non-root user can do most types of administrative tasks after the installation.\nThe commands below can be run as root after cluster node up is successful on the control plane (k3s server) nodes to configure a group called k3sadmin that will allow a non-root user called clouduser to run administrative commands.\ngroupadd k3sadmin\nusermod -aG k3sadmin clouduser\n\nchown root:k3sadmin /usr/local/bin/k3s\nchmod 750 /usr/local/bin/k3s\n\nchown root:k3sadmin /etc/rancher/\nchmod 750 /etc/rancher/\n\nchown -R root:k3sadmin /etc/rancher/k3s/\nchmod 750 /etc/rancher/k3s/\n\nchmod 750 /etc/rancher/k3s/config.yaml\nchmod 660 /etc/rancher/k3s/k3s.yaml\n\n# for ctr\nchown root:k3sadmin /var/lib/rancher/k3s/agent/etc/\nchmod 750 /var/lib/rancher/k3s/agent/etc/\n# for crictl\nchown root:k3sadmin /var/lib/rancher/k3s/agent/etc/crictl.yaml\nchmod 640 /var/lib/rancher/k3s/agent/etc/crictl.yaml\n\n# ONLY for online installs, oc is not available for offline installs\n# for oc\nmkdir -p /home/clouduser/.kube\ncp /etc/rancher/k3s/k3s.yaml /home/clouduser/.kube/config\nchown -R clouduser:clouduser /home/clouduser/.kube\nAfter executing these commands, clouduser will be able to run kubectl, crictl, ctr, aiopsctl, and oc (if online install is used).\n\n‚ùì Where is oc in my offline install?\nThe oc binary is downloaded from openshift.com during installation. For an offline install it assumed that access to openshift.com is not present. But don‚Äôt worry, you can use kubectl instead. :-)\n\n\n\nOffline Installation\nOffline installations, also known as air-gapped deployment, is necessary if the target cluster will not have access to the internet.\n\n‚ùó Security scanning for container images ‚ùó\nIBM does internal security scanning of container images but it is very common for enterprises to require their own security scans for container images prior to allowing installation. THIS CAN BE A VERY LENGTHY PROCESS. Cloud Pak for AIOPs publishes over 400 different container images and every enterprise handles this situation differently.\nIf this is a requirement for your enterprise, get started on this process first as it may quickly become a blocker for the deployment schedule.\n\n‚ùì Where do I find the list of container images ‚ùì\nThe container images for Cloud Pak for AIOps can vary depending on release. There is no way to have the aiopsctl command generate list without also attempting to mirror the images. Here is a hack that will kill the process after it generates the proper manifest file but before the mirroring starts (change the --registry parameter value to match your environment):\n# run aiopsctl bastion login commands for both cp.icr.io and the target registry first\naiopsctl bastion mirror-images --registry artifactory.gym.lan:8443/docker-local 2&gt;&1 | tee &gt;(\n    while read line; do\n        if [[ \"$line\" == *'Mirroring images to registry'* ]]; then\n            echo \"String found. Killing process...\"\n            sleep 5 # time for oc to install\n            pkill -P $$ aiopsctl\n            exit 0\n        fi\n    done\n)\nThis will generate a file called images-mapping.txt under ~/.aiopsctl. The directory will differ depending on the release, but for v4.10.1 the location is ~/.aiopsctl/mirror/.ibm-pak/data/mirror/ibm-cp-waiops/1.16.1\nExample of images-mapping.txt (only first 4 lines shown):\ncp.icr.io/cp/appc/ace-server-prod@sha256:2f9e903bd2cf53ba0878e982ac0f81d956200ffe70a9a57084c2b31bf9134311=artifactory.gym.lan:8443/docker-local/cp/appc/ace-server-prod:13.0.2.2-r2-20250315-121329\ncp.icr.io/cp/appc/acecc-content-server-prod@sha256:4a558be37f530acc1c2395187bbe6d896a2f8dca3c2c9977eb66b7e984805abe=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-content-server-prod:13.0.2.2-r2-20250318-034910\ncp.icr.io/cp/appc/acecc-couchdb3-prod@sha256:c18d850494ba2ee8a90859430e4cd7ea2e6c342eea750afc816f450520dd42c8=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-couchdb3-prod:13.0.2.2-r2-20250318-020015\ncp.icr.io/cp/appc/acecc-dashboard-prod@sha256:ab27d8031bb8ec87a3e465c4e172d9c57cd0b415db78d8305e45f74820949d58=artifactory.gym.lan:8443/docker-local/cp/appc/acecc-dashboard-prod:13.0.2.2-r2-20250325-140612\nFor each line, the section before = is the source container image from IBM container registry.\nTo list only the source images from the images-mapping.txt file, use the following command:\nawk -F= '{print $1}' images-mapping.txt\n\n\n‚ùì Do I really need all of these container images ‚ùì\nMaybe. The container images that make up a release contain back-level images for upgrade purposes. If you are installing AIOps for the first time, you do not need the full set of container images. This is an important detail as security scans might pick up vulnerabilities on container images that are not used in the current version of the product.\nFor example, AIOps might use kafka v3.8.0 and v3.9.0. However, for upgrade purposes kafka v3.6.1 is also be part of the container image manifest. If a security scan finds vulnerabilities in the kafka v3.6.1 image, those can be safely ignore for a new installation as they won‚Äôt be used.\n\n\n‚ùì How do I reduce the image manifest ‚ùì\nWARNING: Be very careful and only attempt this if you know what are you doing. You risk not including a required container image and the below has only been tested on v4.10.1.\nGo to the directoy containing the images-mapping.txt file generated from the bastion mirror-images command. The following command will create a new file called images-mapping-filtered.txt with most of the back-level container images removed.\nawk 'NR==FNR {split($0, a, \"@\"); split($0, b, \":\"); if (b[length(b)] ~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/) versioned_images[a[1]]++; next} {split($0, a, \"@\"); split($0, b, \":\"); if (!(a[1] in versioned_images && b[length(b)] !~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/)) print}' images-mapping.txt images-mapping.txt &gt; images-mapping-filtered.txt\nThere are a few different container types that need to be handled differently (redis, kafka). Run the command below to handle those.\nawk '\n    NR==FNR {\n        if ($0 ~ /ibm-redis-cp-haproxy@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version &gt; max_redis_haproxy_version) max_redis_haproxy_version = version;\n        } else if ($0 ~ /ibm-redis-cp-7.2.7@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version &gt; max_redis_version) max_redis_version = version;\n        } else if ($0 ~ /ibm-events-kafka-[0-9]+\\.[0-9]+\\.[0-9]+@/) {\n            match($0, /ibm-events-kafka-([0-9]+\\.[0-9]+\\.[0-9]+)@/, m);\n            if (m[1] &gt; max1_kafka_version) {\n                max2_kafka_version = max1_kafka_version; max1_kafka_version = m[1];\n            } else if (m[1] &gt; max2_kafka_version) {\n                max2_kafka_version = m[1];\n            }\n        } else {\n            split($0, a, \"@\");\n            split($0, b, \":\");\n            if (b[length(b)] ~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/) {\n                versioned_images[a[1]]++;\n            }\n        }\n        next;\n    }\n    {\n        if ($0 ~ /ibm-redis-cp-haproxy@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version == max_redis_haproxy_version) print;\n        } else if ($0 ~ /ibm-redis-cp-7.2.7@/) {\n            split($0, b, \":\");\n            version = b[length(b)];\n            if (version == max_redis_version) print;\n        } else if ($0 ~ /ibm-events-kafka-[0-9]+\\.[0-9]+\\.[0-9]+@/) {\n            match($0, /ibm-events-kafka-([0-9]+\\.[0-9]+\\.[0-9]+)@/, m);\n            if (m[1] == max1_kafka_version || m[1] == max2_kafka_version) print;\n        } else {\n            split($0, a, \"@\");\n            split($0, b, \":\");\n            if (!(a[1] in versioned_images && b[length(b)] !~ /^v?[0-9]+\\.[0-9]+\\.[0-9]+.*$/)) {\n                print;\n            }\n        }\n    }\n' images-mapping-filtered.txt images-mapping-filtered.txt &gt; images-mapping-filtered.tmp && mv -f images-mapping-filtered.tmp images-mapping-filtered.txt\nYou now have images-mapping-filtered.txt that should be a significantly reduced set of images that will still work for a new installation.\n\n\n‚ùì How do I mirror my reduced images set ‚ùì\nWhen aiops bastion mirror-images runs it creates a set of files that do the mirroring under the covers. Those files are in .aiopsctl/mirror.\nGo to .aiopsctl/mirror and look at oc_image_mirror.sh.\n#!/bin/sh\nset -e\nset -o noglob\n\n# Usage\n#   ENV_VAR=... ./oc_image_mirror.sh &lt;Path to oc binary&gt;\n#\n# Environment variables:\n# - REGISTRY_AUTH_FILE\n#   Your registry authentication file. For Podman, this\n#   is typically `${XDG_RUNTIME_DIR}/containers/auth.json`.\n# - IMAGES_MAPPING_FILE\n#   Your images-mapping.txt to use as the source for mirroring.\n# - DRY_RUN\n#   Set to true to complete a dry-run mirror.\n#\n\n$1 image mirror \\\n-f \"${IMAGES_MAPPING_FILE}\" \\\n-a \"${REGISTRY_AUTH_FILE}\" \\\n--insecure \\\n--filter-by-os '.*' \\\n--skip-multiple-scopes \\\n--max-per-registry=1 \\\n--dry-run=\"${DRY_RUN}\"\nTip: If you want to speed up the mirroring, you can edit this file and change the --max-per-registry=1 to --max-per-registry=6. This will run 6 threads during the mirroring process.\nTo run the mirroring with our reduced images set images-mapping-filtered.txt, run the following command (use the proper path to your file if different).\n# change the path to your file if different\nIMAGES_MAPPING_FILE=./.ibm-pak/data/mirror/ibm-cp-waiops/1.16.1/images-mapping-filtered.txt REGISTRY_AUTH_FILE=./containers/auth.json DRY_RUN=false nohup ./oc_image_mirror.sh ./oc &gt;nohup.out 2&gt;&1 &lt;/dev/null &\nWhile the job is calculating the images to mirror, there will be no output for up to 10 minutes, but then you should see messages showing the mirroring progress in nohup.out. A fully reduced v4.10.1 container set with 6 threads should complete in less than 30 minutes.\n\n\n\nArtifactory\nIf planning for an offline installation using Artifactory as the target registry, ensure that Artifactory meets the requirements for AIOps.\n\nThe target repository layout must be set to Docker\nDocker V2 API must be enabled on the repository\n\n\nüß™ Docker repository validation\nLog in to the Artifactory UI and navigate to the target repository. There will be a type listed for the repository. That type should be Docker.\n\n\n\nüß™ Docker V2 API validation\nTest if the Docker v2 API is enabled by running:\ncurl -u &lt;username&gt;:&lt;password&gt; https://&lt;your-artifactory-domain&gt;/artifactory/&lt;repo-name&gt;/v2/\n\nIf it returns a 200 OK or 401 Unauthorized, the v2 API is enabled.\nIf it returns 404 Not Found, the repo may not support Docker v2.\n\n\n\n‚úÖ Artifactory URL Format\nWhen configuring your container image mirroring to Artifactory, follow these guidelines carefully to ensure compatibility and avoid common pitfalls.\nWhen specifying the Artifactory URL during the mirroring process:\n\n‚ùå Do not include https:// at the beginning.\n‚úÖ Do append :443 to the hostname.\n‚ùå Do not include /artifactory/ in the path like the file URL might display in the Artifactory UI. This is for file downloads only.\n‚úÖ Use the format: &lt;artifactory-host&gt;:443/&lt;repo&gt;/&lt;path-to-images&gt;\n\nFor example, if your Artifactory host is my-artifactory.company.com and your repository is internal-repo, the correct URL might be: my-artifactory.company.com:443/internal-repo/aiops\n\n\nüñ•Ô∏è Platform Requirements\nEnsure that you are running on RHEL 8 or RHEL 9 from the bastion host used to run the mirroring, as the oc version bundled with aiopsctl is compatible with RHEL 8 & 9.\nYou can verify your OS version with:\ncat /etc/redhat-release\nAnd check your oc version with:\noc version",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIOps Handbook",
    "section": "",
    "text": "MD",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#description-aiops-handbook",
    "href": "index.html#description-aiops-handbook",
    "title": "AIOps Handbook",
    "section": "Description: ‚ÄúAIOps Handbook‚Äù",
    "text": "Description: ‚ÄúAIOps Handbook‚Äù",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "AIOps Handbook",
    "section": "The Why",
    "text": "The Why\n(High-level description of the business challenges and client pain points)\n\nProblem Details\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.\n\n\nAdditional Context\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html",
    "href": "src/solution_overview/administration.html",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "This section outlines the procedure for safely performing maintenance (e.g., operating system patching, security updates, or reboots) on K3s cluster nodes that are part of AIOps on Linux. This is a rolling process, meaning only one node is taken offline at a time to maintain service continuity.\n\n\n\nAccess: SSH access to the node(s) to be rebooted.\nPermissions: Kubernetes configuration file (~/.kube/config or root access) for running kubectl commands.\nScript: The k3s-capacity-check.sh script to analyze resource availability.\n\n\n\n\n\nBefore starting any maintenance, you must ensure the remaining nodes have enough guaranteed resources (Memory Requests) to absorb the workload of the node you plan to drain.\n\n\nYou have two options to obtain the script: download it directly from the repository or copy/paste it via the console.\n\n\n\nDownload: Save the script to a server/control-plane node: k3s-capacity-check\nMake Executable: Set the execution permission on the script file:\nchmod +x k3s-capacity-check.sh\n\n\n\n\nClick to expand the section below to view the full script logic. Copy and paste all the contents into a new file named k3s-capacity-check on your target system, then follow step 2 above to make it executable.\n\n\nView Script Source: k3s-capacity-check.sh\n\n#!/bin/bash\n\n# Check if the script is run as root (UID 0)\nif [[ $UID -eq 0 ]]; then\n    # Running as root: use the default kubectl command, assuming necessary environment/configs are set for root.\n    KUBECTL_CMD=\"kubectl\"\n    echo \"Running as root. Using standard 'kubectl'.\"\nelse\n    # Running as non-root: explicitly specify the kubeconfig file.\n    # We use $HOME for the home directory of the current user.\n    KUBECTL_CMD=\"kubectl --kubeconfig $HOME/.kube/config\"\n    echo \"Running as non-root. Using '${KUBECTL_CMD}'.\"\nfi\n\n# Resource to check for the critical bottleneck (Memory is typically the most critical)\nRESOURCE=\"memory\"\n# The scaling factor for Ki to Mi (1024)\nSCALE_FACTOR=1024\n\n# --- Helper Functions (Using AWK for Division) ---\n\n# Function to safely convert any unit (Ki/Mi/Raw Bytes) to MiB (Megabytes)\n# $1: Value string from kubectl (e.g., \"26874661376\", \"29700Mi\", \"38126949Ki\")\nconvert_to_mib() {\n    local val=$1\n    local unit=$(echo \"$val\" | grep -oE '[a-zA-Z]+$')\n    local num=$(echo \"$val\" | grep -oE '^[0-9]+')\n\n    if [[ -z \"$num\" ]]; then\n        echo 0\n        return\n    fi\n\n    # Use awk to handle floating point conversion and rounding\n    if [[ \"$unit\" == \"Ki\" ]]; then\n        # Convert Ki to Mi: Ki / 1024\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1024}'\n    elif [[ \"$unit\" == \"Mi\" ]]; then\n        # Value is already Mi, just echo it\n        echo \"$num\"\n    else\n        # Assume raw bytes if no unit is found, convert Bytes to Mi: Bytes / (1024 * 1024)\n        # Note: We must be cautious with very large byte numbers in awk on some systems.\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1048576}'\n    fi\n}\n\n# --- Data Collection and Calculation ---\n\n# Get a list of all schedulable worker and server nodes\nNODES=$($KUBECTL_CMD get nodes --no-headers -o custom-columns=NAME:.metadata.name)\nNUM_NODES=$(echo \"$NODES\" | wc -l)\n\nTOTAL_CAPACITY_MI=0\nTOTAL_REQUESTS_MI=0\nMAX_NODE_REQUESTS_MI=0\nBUSIEST_NODE=\"\"\n\necho \"--- Kubernetes Cluster Capacity Analysis ---\"\necho \"Using command: ${KUBECTL_CMD}\"\necho \"Analyzing ${NUM_NODES} nodes. Critical Resource: ${RESOURCE^}.\"\necho \"------------------------------------------------\"\n\nfor NODE in $NODES; do\n    # 1. Get Node Capacity (Allocatable)\n    # Extracts the Allocatable memory value (e.g., \"34000Mi\" or \"35000000Ki\")\n    CAPACITY_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk \"/^Allocatable:/{flag=1; next} /${RESOURCE}/ && flag{print \\$2; exit}\" | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    CAPACITY_MI=$(convert_to_mib \"$CAPACITY_VAL\")\n\n    # 2. Get Node Requests\n    # Extracts the Requested memory value (e.g., \"26874661376\", \"29700Mi\")\n    # This AWK command is now carefully structured to grab the *second* field for memory in the \"Allocated resources\" block\n    REQUESTS_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk '/Allocated resources:/,/Events:/{if ($1 == \"memory\") print $2; if ($1 == \"cpu\") print $2}' | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    REQUESTS_MI=$(convert_to_mib \"$REQUESTS_VAL\")\n\n    # Handle cases where request data is missing or failed conversion\n    if [ -z \"$CAPACITY_MI\" ] || [ -z \"$REQUESTS_MI\" ]; then\n        echo \"Warning: Skipped $NODE due to missing data or conversion error.\" &gt;&2\n        continue\n    fi\n\n    # 3. Calculate Totals and Busiest Node (Bash Integer Math)\n    TOTAL_CAPACITY_MI=$((TOTAL_CAPACITY_MI + CAPACITY_MI))\n    TOTAL_REQUESTS_MI=$((TOTAL_REQUESTS_MI + REQUESTS_MI))\n\n    if [ \"$REQUESTS_MI\" -gt \"$MAX_NODE_REQUESTS_MI\" ]; then\n        MAX_NODE_REQUESTS_MI=\"$REQUESTS_MI\"\n        BUSIEST_NODE=\"$NODE\"\n    fi\ndone\n\n# --- Final Calculations and Summary Output (Bash Integer Math) ---\nFREE_CAPACITY_MI=$((TOTAL_CAPACITY_MI - TOTAL_REQUESTS_MI))\nNET_CAPACITY_AFTER_DRAIN=$((FREE_CAPACITY_MI - MAX_NODE_REQUESTS_MI))\n\necho \"------------------------------------------------\"\necho \"--- Cluster Totals (for ${RESOURCE^}) ---\"\necho \"Total Cluster Allocatable: $((TOTAL_CAPACITY_MI / 1024)) Gi\"\necho \"Total Cluster Requests:    $((TOTAL_REQUESTS_MI / 1024)) Gi\"\necho \"Total Cluster Free Capacity: $((FREE_CAPACITY_MI / 1024)) Gi\"\necho \"------------------------------------------------\"\necho \"--- Maintenance Prediction ---\"\n\nif [ \"$NET_CAPACITY_AFTER_DRAIN\" -ge 0 ]; then\n    echo \"‚úÖ PREDICTION: SUCCESSFUL\"\n    echo \"The cluster has enough guaranteed capacity (Memory) to absorb the busiest node's workload.\"\n    echo \"Remaining Free Capacity after draining busiest node: $((NET_CAPACITY_AFTER_DRAIN / 1024)) Gi\"\nelse\n    echo \"üö® PREDICTION: FAILURE RISK\"\n    echo \"The cluster does NOT have enough guaranteed free capacity (Memory) to absorb the busiest node's workload.\"\n    # Use integer math for the negative result and division\n    CAPACITY_SHORTFALL=$((-NET_CAPACITY_AFTER_DRAIN))\n    echo \"Capacity Shortfall: $((CAPACITY_SHORTFALL / 1024)) Gi\"\nfi\n\necho \"------------------------------------------------\"\necho \"HIGHEST RISK NODE (If drained): $BUSIEST_NODE\"\necho \"Load to be re-scheduled: $((MAX_NODE_REQUESTS_MI / 1024)) Gi\"\necho \"\"\n\n\n\n\n\nExecute the script to determine if you have enough capacity for a safe drain:\n./k3s-capacity-check.sh\n\nAnalyze the Prediction:\n\n‚úÖ SUCCESSFUL: The Remaining Free Capacity is positive. Proceed to Step 2.\nüö® FAILURE RISK: The script reports a Capacity Shortfall. DO NOT PROCEED. Scale up your cluster until the script predicts success.\n\n\n\n\n\n\n\nThis step removes the node‚Äôs workload and prevents new Pods from being scheduled to it.\n\nIdentify the Target Node: Choose the safest node first (the one with the lowest ‚ÄúLoad to be re-scheduled‚Äù as reported by the script) or the first node in your rotation.\nCordon and Drain the Node: Run the kubectl drain command. This automatically marks the node as Unschedulable (Cordon) and safely evicts all running Pods.\n# Replace &lt;node-name&gt; with the actual node name, e.g., aiops-k3s-agent-0.gym.lan\nkubectl drain &lt;node-name&gt; \\\n  --ignore-daemonsets \\\n  --delete-emptydir-data\n\n\n\n\n\n\n\nFlag\nPurpose\n\n\n\n\n--ignore-daemonsets\nEnsures critical cluster services (managed by DaemonSets) are not evicted.\n\n\n--delete-emptydir-data\nRequired to evict Pods using EmptyDir volumes (which are temporary and local).\n\n\n\nVerify the Drain: Confirm the node status and that no user Pods remain.\nkubectl get nodes\n# Status should show \"&lt;node-name&gt; Ready,SchedulingDisabled\"\n\n\n\n\n\nOnce the node is drained, it is safe to perform the necessary operating system work.\n\nSSH and Update: SSH into the drained node and perform OS patching/updates.\nssh &lt;node-name&gt;\n# e.g., sudo apt update && sudo apt upgrade -y\nReboot: Reboot the server to finalize updates.\nsudo reboot\nWait for Ready State: Wait until the node reboots and its status changes back to Ready (but still SchedulingDisabled). This may take a few minutes as K3s/kubelet starts up.\n# Run this periodically from another node\nkubectl get nodes\n\n\n\n\n\n\nUncordon the Node: Mark the node as Schedulable so the Kubernetes scheduler can once again place Pods onto it.\nkubectl uncordon &lt;node-name&gt;\nVerify Pod Rescheduling: Check that the Pods evicted during the drain process have successfully rescheduled themselves across the cluster, including back onto the newly available node.\nVerify Cluster Health: Check the health of the entire cluster before moving on to the next node.\nkubectl get pods -A | grep -v 'Running'\n# Ensure no Pods are stuck in a Pending or CrashLoopBackOff state.\n\nRepeat Steps 2 through 4 for the next node in the maintenance cycle. Always re-verify capacity if there have been significant workload changes.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#prerequisites",
    "href": "src/solution_overview/administration.html#prerequisites",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Access: SSH access to the node(s) to be rebooted.\nPermissions: Kubernetes configuration file (~/.kube/config or root access) for running kubectl commands.\nScript: The k3s-capacity-check.sh script to analyze resource availability.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-1-verify-cluster-capacity-before-starting",
    "href": "src/solution_overview/administration.html#step-1-verify-cluster-capacity-before-starting",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Before starting any maintenance, you must ensure the remaining nodes have enough guaranteed resources (Memory Requests) to absorb the workload of the node you plan to drain.\n\n\nYou have two options to obtain the script: download it directly from the repository or copy/paste it via the console.\n\n\n\nDownload: Save the script to a server/control-plane node: k3s-capacity-check\nMake Executable: Set the execution permission on the script file:\nchmod +x k3s-capacity-check.sh\n\n\n\n\nClick to expand the section below to view the full script logic. Copy and paste all the contents into a new file named k3s-capacity-check on your target system, then follow step 2 above to make it executable.\n\n\nView Script Source: k3s-capacity-check.sh\n\n#!/bin/bash\n\n# Check if the script is run as root (UID 0)\nif [[ $UID -eq 0 ]]; then\n    # Running as root: use the default kubectl command, assuming necessary environment/configs are set for root.\n    KUBECTL_CMD=\"kubectl\"\n    echo \"Running as root. Using standard 'kubectl'.\"\nelse\n    # Running as non-root: explicitly specify the kubeconfig file.\n    # We use $HOME for the home directory of the current user.\n    KUBECTL_CMD=\"kubectl --kubeconfig $HOME/.kube/config\"\n    echo \"Running as non-root. Using '${KUBECTL_CMD}'.\"\nfi\n\n# Resource to check for the critical bottleneck (Memory is typically the most critical)\nRESOURCE=\"memory\"\n# The scaling factor for Ki to Mi (1024)\nSCALE_FACTOR=1024\n\n# --- Helper Functions (Using AWK for Division) ---\n\n# Function to safely convert any unit (Ki/Mi/Raw Bytes) to MiB (Megabytes)\n# $1: Value string from kubectl (e.g., \"26874661376\", \"29700Mi\", \"38126949Ki\")\nconvert_to_mib() {\n    local val=$1\n    local unit=$(echo \"$val\" | grep -oE '[a-zA-Z]+$')\n    local num=$(echo \"$val\" | grep -oE '^[0-9]+')\n\n    if [[ -z \"$num\" ]]; then\n        echo 0\n        return\n    fi\n\n    # Use awk to handle floating point conversion and rounding\n    if [[ \"$unit\" == \"Ki\" ]]; then\n        # Convert Ki to Mi: Ki / 1024\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1024}'\n    elif [[ \"$unit\" == \"Mi\" ]]; then\n        # Value is already Mi, just echo it\n        echo \"$num\"\n    else\n        # Assume raw bytes if no unit is found, convert Bytes to Mi: Bytes / (1024 * 1024)\n        # Note: We must be cautious with very large byte numbers in awk on some systems.\n        echo \"$num\" | awk '{printf \"%.0f\", $1 / 1048576}'\n    fi\n}\n\n# --- Data Collection and Calculation ---\n\n# Get a list of all schedulable worker and server nodes\nNODES=$($KUBECTL_CMD get nodes --no-headers -o custom-columns=NAME:.metadata.name)\nNUM_NODES=$(echo \"$NODES\" | wc -l)\n\nTOTAL_CAPACITY_MI=0\nTOTAL_REQUESTS_MI=0\nMAX_NODE_REQUESTS_MI=0\nBUSIEST_NODE=\"\"\n\necho \"--- Kubernetes Cluster Capacity Analysis ---\"\necho \"Using command: ${KUBECTL_CMD}\"\necho \"Analyzing ${NUM_NODES} nodes. Critical Resource: ${RESOURCE^}.\"\necho \"------------------------------------------------\"\n\nfor NODE in $NODES; do\n    # 1. Get Node Capacity (Allocatable)\n    # Extracts the Allocatable memory value (e.g., \"34000Mi\" or \"35000000Ki\")\n    CAPACITY_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk \"/^Allocatable:/{flag=1; next} /${RESOURCE}/ && flag{print \\$2; exit}\" | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    CAPACITY_MI=$(convert_to_mib \"$CAPACITY_VAL\")\n\n    # 2. Get Node Requests\n    # Extracts the Requested memory value (e.g., \"26874661376\", \"29700Mi\")\n    # This AWK command is now carefully structured to grab the *second* field for memory in the \"Allocated resources\" block\n    REQUESTS_VAL=$($KUBECTL_CMD describe node \"$NODE\" | awk '/Allocated resources:/,/Events:/{if ($1 == \"memory\") print $2; if ($1 == \"cpu\") print $2}' | grep -oE '^[0-9]+(Mi|Ki)?$')\n\n    # Convert to MiB using the helper function\n    REQUESTS_MI=$(convert_to_mib \"$REQUESTS_VAL\")\n\n    # Handle cases where request data is missing or failed conversion\n    if [ -z \"$CAPACITY_MI\" ] || [ -z \"$REQUESTS_MI\" ]; then\n        echo \"Warning: Skipped $NODE due to missing data or conversion error.\" &gt;&2\n        continue\n    fi\n\n    # 3. Calculate Totals and Busiest Node (Bash Integer Math)\n    TOTAL_CAPACITY_MI=$((TOTAL_CAPACITY_MI + CAPACITY_MI))\n    TOTAL_REQUESTS_MI=$((TOTAL_REQUESTS_MI + REQUESTS_MI))\n\n    if [ \"$REQUESTS_MI\" -gt \"$MAX_NODE_REQUESTS_MI\" ]; then\n        MAX_NODE_REQUESTS_MI=\"$REQUESTS_MI\"\n        BUSIEST_NODE=\"$NODE\"\n    fi\ndone\n\n# --- Final Calculations and Summary Output (Bash Integer Math) ---\nFREE_CAPACITY_MI=$((TOTAL_CAPACITY_MI - TOTAL_REQUESTS_MI))\nNET_CAPACITY_AFTER_DRAIN=$((FREE_CAPACITY_MI - MAX_NODE_REQUESTS_MI))\n\necho \"------------------------------------------------\"\necho \"--- Cluster Totals (for ${RESOURCE^}) ---\"\necho \"Total Cluster Allocatable: $((TOTAL_CAPACITY_MI / 1024)) Gi\"\necho \"Total Cluster Requests:    $((TOTAL_REQUESTS_MI / 1024)) Gi\"\necho \"Total Cluster Free Capacity: $((FREE_CAPACITY_MI / 1024)) Gi\"\necho \"------------------------------------------------\"\necho \"--- Maintenance Prediction ---\"\n\nif [ \"$NET_CAPACITY_AFTER_DRAIN\" -ge 0 ]; then\n    echo \"‚úÖ PREDICTION: SUCCESSFUL\"\n    echo \"The cluster has enough guaranteed capacity (Memory) to absorb the busiest node's workload.\"\n    echo \"Remaining Free Capacity after draining busiest node: $((NET_CAPACITY_AFTER_DRAIN / 1024)) Gi\"\nelse\n    echo \"üö® PREDICTION: FAILURE RISK\"\n    echo \"The cluster does NOT have enough guaranteed free capacity (Memory) to absorb the busiest node's workload.\"\n    # Use integer math for the negative result and division\n    CAPACITY_SHORTFALL=$((-NET_CAPACITY_AFTER_DRAIN))\n    echo \"Capacity Shortfall: $((CAPACITY_SHORTFALL / 1024)) Gi\"\nfi\n\necho \"------------------------------------------------\"\necho \"HIGHEST RISK NODE (If drained): $BUSIEST_NODE\"\necho \"Load to be re-scheduled: $((MAX_NODE_REQUESTS_MI / 1024)) Gi\"\necho \"\"\n\n\n\n\n\nExecute the script to determine if you have enough capacity for a safe drain:\n./k3s-capacity-check.sh\n\nAnalyze the Prediction:\n\n‚úÖ SUCCESSFUL: The Remaining Free Capacity is positive. Proceed to Step 2.\nüö® FAILURE RISK: The script reports a Capacity Shortfall. DO NOT PROCEED. Scale up your cluster until the script predicts success.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-2-prepare-the-node-for-maintenance-cordon-and-drain",
    "href": "src/solution_overview/administration.html#step-2-prepare-the-node-for-maintenance-cordon-and-drain",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "This step removes the node‚Äôs workload and prevents new Pods from being scheduled to it.\n\nIdentify the Target Node: Choose the safest node first (the one with the lowest ‚ÄúLoad to be re-scheduled‚Äù as reported by the script) or the first node in your rotation.\nCordon and Drain the Node: Run the kubectl drain command. This automatically marks the node as Unschedulable (Cordon) and safely evicts all running Pods.\n# Replace &lt;node-name&gt; with the actual node name, e.g., aiops-k3s-agent-0.gym.lan\nkubectl drain &lt;node-name&gt; \\\n  --ignore-daemonsets \\\n  --delete-emptydir-data\n\n\n\n\n\n\n\nFlag\nPurpose\n\n\n\n\n--ignore-daemonsets\nEnsures critical cluster services (managed by DaemonSets) are not evicted.\n\n\n--delete-emptydir-data\nRequired to evict Pods using EmptyDir volumes (which are temporary and local).\n\n\n\nVerify the Drain: Confirm the node status and that no user Pods remain.\nkubectl get nodes\n# Status should show \"&lt;node-name&gt; Ready,SchedulingDisabled\"",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-3-perform-maintenance-patch-and-reboot",
    "href": "src/solution_overview/administration.html#step-3-perform-maintenance-patch-and-reboot",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Once the node is drained, it is safe to perform the necessary operating system work.\n\nSSH and Update: SSH into the drained node and perform OS patching/updates.\nssh &lt;node-name&gt;\n# e.g., sudo apt update && sudo apt upgrade -y\nReboot: Reboot the server to finalize updates.\nsudo reboot\nWait for Ready State: Wait until the node reboots and its status changes back to Ready (but still SchedulingDisabled). This may take a few minutes as K3s/kubelet starts up.\n# Run this periodically from another node\nkubectl get nodes",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/administration.html#step-4-complete-the-cycle-uncordon-and-verify",
    "href": "src/solution_overview/administration.html#step-4-complete-the-cycle-uncordon-and-verify",
    "title": "AIOps on Linux Administration",
    "section": "",
    "text": "Uncordon the Node: Mark the node as Schedulable so the Kubernetes scheduler can once again place Pods onto it.\nkubectl uncordon &lt;node-name&gt;\nVerify Pod Rescheduling: Check that the Pods evicted during the drain process have successfully rescheduled themselves across the cluster, including back onto the newly available node.\nVerify Cluster Health: Check the health of the entire cluster before moving on to the next node.\nkubectl get pods -A | grep -v 'Running'\n# Ensure no Pods are stuck in a Pending or CrashLoopBackOff state.\n\nRepeat Steps 2 through 4 for the next node in the maintenance cycle. Always re-verify capacity if there have been significant workload changes.",
    "crumbs": [
      "Solution Overview",
      "Administration"
    ]
  },
  {
    "objectID": "src/solution_overview/deploy.html",
    "href": "src/solution_overview/deploy.html",
    "title": "AIOps on Linux Deployment",
    "section": "",
    "text": "Under Construction",
    "crumbs": [
      "Solution Overview",
      "Deploy"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html",
    "href": "src/implementation_methodology/stepone-imp.html",
    "title": "Step One",
    "section": "",
    "text": "Step One Implementation - The How\n(Details the process and techniques used to execute the technical solution)"
  },
  {
    "objectID": "src/implementation_methodology/stepthree-imp.html",
    "href": "src/implementation_methodology/stepthree-imp.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  }
]